import os
import numpy as np
import matplotlib.pyplot as plt
import pickle
from pathlib import Path
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator


class NeuralNetworkTrainer:
    """
    Sistema completo de entrenamiento para clasificador de animales basado en CNN.
    
    Esta clase encapsula todo el proceso de entrenamiento incluyendo:
    - Carga y preprocesamiento de datos
    - ConstrucciÃ³n de arquitectura CNN
    - Entrenamiento con callbacks adaptativos
    - Persistencia de modelos y mÃ©tricas
    - VisualizaciÃ³n de resultados
    """
    
    # ===== CONFIGURACIÃ“N DE HIPERPARÃMETROS =====
    INPUT_DIMENSIONS = 128          # ResoluciÃ³n de entrada para la red
    COLOR_CHANNELS = 3              # RGB (3 canales de color)
    SAMPLES_PER_BATCH = 32          # TamaÃ±o de lote para entrenamiento
    TRAINING_ITERATIONS = 45        # NÃºmero mÃ¡ximo de Ã©pocas
    INITIAL_LEARNING_RATE = 0.001   # Tasa de aprendizaje inicial
    VALIDATION_SPLIT_RATIO = 0.2    # 20% de datos para validaciÃ³n
    
    # ConfiguraciÃ³n de regularizaciÃ³n
    DROPOUT_LIGHT = 0.3             # Dropout suave en capas convolucionales
    DROPOUT_HEAVY = 0.5             # Dropout agresivo antes de clasificaciÃ³n
    
    # ConfiguraciÃ³n de callbacks
    LR_REDUCTION_FACTOR = 0.5       # Factor de reducciÃ³n del learning rate
    EARLY_STOP_PATIENCE = 5         # Ã‰pocas de paciencia para early stopping
    LR_PLATEAU_PATIENCE = 3         # Ã‰pocas de paciencia para reducir LR
    
    def __init__(self, dataset_directory):
        """
        Inicializa el sistema de entrenamiento.
        
        Args:
            dataset_directory (str): Ruta al directorio que contiene las carpetas
                                     de clases con las imÃ¡genes de entrenamiento
        """
        self.dataset_root = dataset_directory
        self.neural_model = None
        self.training_history = None
        self.category_labels = None
        self.total_categories = 0
        self.train_data_flow = None
        self.validation_data_flow = None
        
        print(f"\n{'='*70}")
        print(f"  INICIALIZANDO SISTEMA DE ENTRENAMIENTO CNN")
        print(f"{'='*70}")
        print(f"Dataset: {self.dataset_root}")
    
    def setup_data_generators(self):
        """
        Configura los generadores de datos con augmentaciÃ³n y normalizaciÃ³n.
        
        La augmentaciÃ³n de datos incluye:
        - Rotaciones aleatorias (Â±20Â°)
        - Desplazamientos horizontales/verticales (Â±20%)
        - Volteo horizontal (flip)
        - Zoom aleatorio (Â±20%)
        
        Esto previene overfitting al aumentar artificialmente la variedad del dataset.
        """
        print(f"\n[PASO 1] Configurando pipeline de datos...")
        
        # Pipeline de augmentaciÃ³n para datos de entrenamiento
        # Cada imagen se transforma aleatoriamente en cada Ã©poca
        augmentation_pipeline = ImageDataGenerator(
            rescale=1./255,                          # Normaliza pÃ­xeles a [0, 1]
            validation_split=self.VALIDATION_SPLIT_RATIO,
            rotation_range=20,                       # RotaciÃ³n aleatoria Â±20Â°
            width_shift_range=0.2,                   # Desplazamiento horizontal Â±20%
            height_shift_range=0.2,                  # Desplazamiento vertical Â±20%
            horizontal_flip=True,                    # Espejo horizontal aleatorio
            zoom_range=0.2                           # Zoom aleatorio Â±20%
        )
        
        # Pipeline simple para validaciÃ³n (solo normalizaciÃ³n)
        validation_pipeline = ImageDataGenerator(rescale=1./255)
        
        # Crear flujo de datos de entrenamiento desde directorios
        self.train_data_flow = augmentation_pipeline.flow_from_directory(
            self.dataset_root,
            target_size=(self.INPUT_DIMENSIONS, self.INPUT_DIMENSIONS),
            batch_size=self.SAMPLES_PER_BATCH,
            class_mode='categorical',               # One-hot encoding para mÃºltiples clases
            subset='training',                      # Usar el 80% para entrenamiento
            shuffle=True,                           # Mezclar muestras en cada Ã©poca
            interpolation='bilinear',               # MÃ©todo de redimensionamiento suave
            keep_aspect_ratio=False                 # Forzar tamaÃ±o exacto (puede deformar)
        )
        
        # Crear flujo de datos de validaciÃ³n
        self.validation_data_flow = augmentation_pipeline.flow_from_directory(
            self.dataset_root,
            target_size=(self.INPUT_DIMENSIONS, self.INPUT_DIMENSIONS),
            batch_size=self.SAMPLES_PER_BATCH,
            class_mode='categorical',
            subset='validation',                    # Usar el 20% para validaciÃ³n
            shuffle=False,                          # No mezclar validaciÃ³n (reproducibilidad)
            interpolation='bilinear',
            keep_aspect_ratio=False
        )
        
        # Extraer metadatos del dataset
        self.category_labels = list(self.train_data_flow.class_indices.keys())
        self.total_categories = len(self.category_labels)
        
        # Reportar estadÃ­sticas del dataset
        print(f"\n{'â”€'*70}")
        print(f"  ESTADÃSTICAS DEL DATASET")
        print(f"{'â”€'*70}")
        print(f"ğŸ“ CategorÃ­as detectadas: {self.category_labels}")
        print(f"ğŸ”¢ Total de categorÃ­as: {self.total_categories}")
        print(f"ğŸ¯ Muestras de entrenamiento: {self.train_data_flow.samples}")
        print(f"âœ… Muestras de validaciÃ³n: {self.validation_data_flow.samples}")
        print(f"{'â”€'*70}\n")
    
    def construct_network_architecture(self):
        """
        Construye la arquitectura de la Red Neuronal Convolucional.
        
        Arquitectura de 4 bloques convolucionales con complejidad creciente:
        
        Bloque 1 (128x128 â†’ 64x64): Detecta caracterÃ­sticas bÃ¡sicas
            - 32 filtros: Detecta bordes, colores, gradientes simples
            
        Bloque 2 (64x64 â†’ 32x32): Detecta patrones de nivel medio
            - 64 filtros: Detecta texturas, formas geomÃ©tricas bÃ¡sicas
            
        Bloque 3 (32x32 â†’ 16x16): Detecta estructuras complejas
            - 128 filtros: Detecta partes de animales (orejas, patas, colas)
            
        Bloque 4 (16x16 â†’ 8x8): Detecta objetos completos
            - 128 filtros: Detecta animales completos y contextos
        
        Clasificador final: Capas densas para la decisiÃ³n
            - 512 neuronas: IntegraciÃ³n de caracterÃ­sticas
            - 128 neuronas: AbstracciÃ³n final
            - N neuronas: Probabilidades por categorÃ­a (softmax)
        """
        print(f"[PASO 2] Construyendo arquitectura de red neuronal...")
        
        self.neural_model = Sequential(name='AnimalClassifierCNN')
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BLOQUE CONVOLUCIONAL 1: ExtracciÃ³n de caracterÃ­sticas primitivas
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ResoluciÃ³n: 128x128 â†’ 64x64
        self.neural_model.add(Conv2D(
            filters=32,                              # 32 detectores de patrones
            kernel_size=(3, 3),                      # Ventana de anÃ¡lisis 3x3
            padding='same',                          # Mantiene dimensiones
            input_shape=(self.INPUT_DIMENSIONS, self.INPUT_DIMENSIONS, self.COLOR_CHANNELS),
            name='conv_layer_1_basic_features'
        ))
        self.neural_model.add(LeakyReLU(alpha=0.1, name='activation_1'))  # Evita "neuronas muertas"
        self.neural_model.add(MaxPooling2D(pool_size=(2, 2), name='pooling_1'))  # Reduce a la mitad
        self.neural_model.add(Dropout(rate=self.DROPOUT_LIGHT, name='dropout_1'))  # Previene overfitting
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BLOQUE CONVOLUCIONAL 2: Patrones de nivel medio
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ResoluciÃ³n: 64x64 â†’ 32x32
        self.neural_model.add(Conv2D(
            filters=64,
            kernel_size=(3, 3),
            padding='same',
            name='conv_layer_2_textures'
        ))
        self.neural_model.add(LeakyReLU(alpha=0.1, name='activation_2'))
        self.neural_model.add(MaxPooling2D(pool_size=(2, 2), name='pooling_2'))
        self.neural_model.add(Dropout(rate=self.DROPOUT_LIGHT, name='dropout_2'))
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BLOQUE CONVOLUCIONAL 3: Estructuras complejas
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ResoluciÃ³n: 32x32 â†’ 16x16
        # Incremento a 128 filtros para capturar mayor complejidad biolÃ³gica
        self.neural_model.add(Conv2D(
            filters=128,
            kernel_size=(3, 3),
            padding='same',
            name='conv_layer_3_parts'
        ))
        self.neural_model.add(LeakyReLU(alpha=0.1, name='activation_3'))
        self.neural_model.add(MaxPooling2D(pool_size=(2, 2), name='pooling_3'))
        self.neural_model.add(Dropout(rate=self.DROPOUT_LIGHT, name='dropout_3'))
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # BLOQUE CONVOLUCIONAL 4: Objetos completos
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ResoluciÃ³n: 16x16 â†’ 8x8
        self.neural_model.add(Conv2D(
            filters=128,
            kernel_size=(3, 3),
            padding='same',
            name='conv_layer_4_objects'
        ))
        self.neural_model.add(LeakyReLU(alpha=0.1, name='activation_4'))
        self.neural_model.add(MaxPooling2D(pool_size=(2, 2), name='pooling_4'))
        self.neural_model.add(Dropout(rate=self.DROPOUT_LIGHT, name='dropout_4'))
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CLASIFICADOR DENSO: Toma de decisiÃ³n final
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.neural_model.add(Flatten(name='flatten'))  # Convierte mapas 2D a vector 1D
        
        # Capa de integraciÃ³n: Combina todas las caracterÃ­sticas extraÃ­das
        self.neural_model.add(Dense(units=512, name='dense_integration'))
        self.neural_model.add(LeakyReLU(alpha=0.1, name='activation_integration'))
        self.neural_model.add(Dropout(rate=self.DROPOUT_HEAVY, name='dropout_heavy'))  # RegularizaciÃ³n agresiva
        
        # Capa de abstracciÃ³n: RepresentaciÃ³n de alto nivel
        self.neural_model.add(Dense(units=128, name='dense_abstraction'))
        self.neural_model.add(LeakyReLU(alpha=0.1, name='activation_abstraction'))
        
        # Capa de salida: Probabilidades por categorÃ­a
        self.neural_model.add(Dense(
            units=self.total_categories,
            activation='softmax',                    # Convierte scores a probabilidades
            name='output_probabilities'
        ))
        
        # Mostrar resumen de la arquitectura
        print(f"\n{'â”€'*70}")
        print(f"  ARQUITECTURA DE LA RED")
        print(f"{'â”€'*70}")
        self.neural_model.summary()
        print(f"{'â”€'*70}\n")
    
    def configure_training_strategy(self):
        """
        Configura el proceso de optimizaciÃ³n y las estrategias de entrenamiento adaptativo.
        
        Utiliza:
        - Adam optimizer: Algoritmo de optimizaciÃ³n adaptativo que ajusta
          los pesos de la red para minimizar el error
        - Categorical crossentropy: FunciÃ³n de pÃ©rdida para clasificaciÃ³n multiclase
        - ReduceLROnPlateau: Reduce el learning rate cuando el progreso se estanca
        - EarlyStopping: Detiene el entrenamiento si no hay mejoras significativas
        """
        print(f"[PASO 3] Configurando estrategia de optimizaciÃ³n...")
        
        # Optimizador Adam: Ajusta pesos y sesgos de manera eficiente
        # Combina momentum y tasa de aprendizaje adaptativa
        optimization_algorithm = Adam(learning_rate=self.INITIAL_LEARNING_RATE)
        
        # Compilar el modelo con funciÃ³n de pÃ©rdida y mÃ©trica de evaluaciÃ³n
        self.neural_model.compile(
            loss='categorical_crossentropy',         # PÃ©rdida para clasificaciÃ³n multiclase
            optimizer=optimization_algorithm,
            metrics=['accuracy']                     # MÃ©trica principal: precisiÃ³n
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Callback 1: ReducciÃ³n adaptativa del learning rate
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Cuando la red deja de mejorar, reduce el LR para hacer ajustes mÃ¡s finos
        self.lr_scheduler = ReduceLROnPlateau(
            monitor='val_loss',                      # Observa la pÃ©rdida de validaciÃ³n
            factor=self.LR_REDUCTION_FACTOR,         # Reduce LR a la mitad
            patience=self.LR_PLATEAU_PATIENCE,       # Espera 3 Ã©pocas sin mejora
            min_lr=1e-7,                             # LR mÃ­nimo permitido
            verbose=1,                               # Mostrar cuando se active
            mode='min'                               # Queremos minimizar la pÃ©rdida
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Callback 2: DetenciÃ³n temprana
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Si no hay mejoras significativas, detiene el entrenamiento
        # y restaura los mejores pesos encontrados
        self.early_termination = EarlyStopping(
            monitor='val_loss',                      # Observa la pÃ©rdida de validaciÃ³n
            patience=self.EARLY_STOP_PATIENCE,       # Espera 5 Ã©pocas sin mejora
            min_delta=0.001,                         # Mejora mÃ­nima considerada significativa
            restore_best_weights=True,               # Vuelve a los mejores pesos
            verbose=1,                               # Mostrar cuando se active
            mode='min'
        )
        
        print(f"âœ“ Optimizador configurado: Adam (LR={self.INITIAL_LEARNING_RATE})")
        print(f"âœ“ FunciÃ³n de pÃ©rdida: Categorical Crossentropy")
        print(f"âœ“ Callbacks activos: ReduceLROnPlateau, EarlyStopping\n")
    
    def execute_training(self):
        """
        Ejecuta el proceso completo de entrenamiento con validaciÃ³n.
        
        El entrenamiento procede en Ã©pocas, donde cada Ã©poca:
        1. Procesa todos los batches de entrenamiento
        2. EvalÃºa el rendimiento en el conjunto de validaciÃ³n
        3. Los callbacks ajustan parÃ¡metros o detienen si es necesario
        """
        print(f"[PASO 4] Iniciando proceso de entrenamiento...")
        print(f"\n{'â•'*70}")
        print(f"  ENTRENAMIENTO EN PROGRESO")
        print(f"{'â•'*70}\n")
        
        # Entrenar el modelo usando generadores
        self.training_history = self.neural_model.fit(
            self.train_data_flow,                    # Generador de datos de entrenamiento
            steps_per_epoch=len(self.train_data_flow),  # Pasos = total_muestras / batch_size
            epochs=self.TRAINING_ITERATIONS,         # NÃºmero mÃ¡ximo de iteraciones
            verbose=1,                               # Mostrar barra de progreso
            validation_data=self.validation_data_flow,  # Datos para evaluar generalizaciÃ³n
            validation_steps=len(self.validation_data_flow),
            callbacks=[self.lr_scheduler, self.early_termination]  # Estrategias adaptativas
        )
        
        # Extraer la mejor mÃ©trica de validaciÃ³n alcanzada
        best_validation_accuracy = max(self.training_history.history['val_accuracy']) * 100
        
        print(f"\n{'â•'*70}")
        print(f"  ENTRENAMIENTO COMPLETADO")
        print(f"{'â•'*70}")
        print(f"ğŸ† Mejor precisiÃ³n en validaciÃ³n: {best_validation_accuracy:.2f}%")
        print(f"{'â•'*70}\n")
    
    def persist_model_and_metrics(self, model_filename="animal_classifier_optimized-2.h5"):
        """
        Guarda el modelo entrenado y el historial de mÃ©tricas en disco.
        
        Args:
            model_filename (str): Nombre del archivo para guardar el modelo
        """
        print(f"[PASO 5] Persistiendo resultados del entrenamiento...")
        
        # Guardar el modelo completo (arquitectura + pesos + optimizador)
        self.neural_model.save(model_filename)
        print(f"âœ“ Modelo guardado: {model_filename}")
        
        # Guardar historial de entrenamiento (mÃ©tricas por Ã©poca)
        history_file = "training_history.pkl"
        with open(history_file, 'wb') as file_handler:
            pickle.dump(self.training_history.history, file_handler)
        print(f"âœ“ Historial de mÃ©tricas guardado: {history_file}\n")
    
    def visualize_training_results(self):
        """
        Genera y guarda visualizaciones del progreso del entrenamiento.
        
        Crea dos grÃ¡ficos:
        1. EvoluciÃ³n de la precisiÃ³n (training vs validation)
        2. EvoluciÃ³n de la pÃ©rdida (training vs validation)
        
        Ãštil para diagnosticar overfitting, underfitting y convergencia.
        """
        print(f"[PASO 6] Generando visualizaciones...")
        
        # Extraer mÃ©tricas del historial
        training_accuracy = self.training_history.history['accuracy']
        validation_accuracy = self.training_history.history['val_accuracy']
        training_loss = self.training_history.history['loss']
        validation_loss = self.training_history.history['val_loss']
        epoch_indices = range(1, len(training_accuracy) + 1)
        
        # Crear figura con dos subplots lado a lado
        figure, axes = plt.subplots(1, 2, figsize=(14, 6))
        figure.suptitle('AnÃ¡lisis de Rendimiento del Entrenamiento', fontsize=16, fontweight='bold')
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Subplot 1: EvoluciÃ³n de la PrecisiÃ³n
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        axes[0].plot(epoch_indices, training_accuracy, 
                     label='PrecisiÃ³n en Entrenamiento', 
                     color='#2E86DE', linewidth=2, marker='o', markersize=4)
        axes[0].plot(epoch_indices, validation_accuracy, 
                     label='PrecisiÃ³n en ValidaciÃ³n', 
                     color='#10AC84', linewidth=2, marker='s', markersize=4)
        axes[0].set_xlabel('Ã‰poca', fontsize=12)
        axes[0].set_ylabel('PrecisiÃ³n', fontsize=12)
        axes[0].set_title('EvoluciÃ³n de PrecisiÃ³n', fontsize=14, fontweight='bold')
        axes[0].legend(loc='lower right')
        axes[0].grid(True, alpha=0.3, linestyle='--')
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Subplot 2: EvoluciÃ³n de la PÃ©rdida
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        axes[1].plot(epoch_indices, training_loss, 
                     label='PÃ©rdida en Entrenamiento', 
                     color='#EE5A6F', linewidth=2, marker='o', markersize=4)
        axes[1].plot(epoch_indices, validation_loss, 
                     label='PÃ©rdida en ValidaciÃ³n', 
                     color='#FC5C65', linewidth=2, marker='s', markersize=4)
        axes[1].set_xlabel('Ã‰poca', fontsize=12)
        axes[1].set_ylabel('PÃ©rdida', fontsize=12)
        axes[1].set_title('EvoluciÃ³n de PÃ©rdida', fontsize=14, fontweight='bold')
        axes[1].legend(loc='upper right')
        axes[1].grid(True, alpha=0.3, linestyle='--')
        
        # Ajustar layout y guardar
        plt.tight_layout()
        output_filename = 'training_results.png'
        plt.savefig(output_filename, dpi=300, bbox_inches='tight')
        print(f"âœ“ GrÃ¡ficas guardadas: {output_filename}")
        
        # Mostrar las grÃ¡ficas
        plt.show()
        
        print(f"\n{'='*70}")
        print(f"  PROCESO COMPLETO FINALIZADO CON Ã‰XITO")
        print(f"{'='*70}\n")
    
    def run_complete_pipeline(self):
        """
        Ejecuta el pipeline completo de entrenamiento de principio a fin.
        
        Secuencia de ejecuciÃ³n:
        1. Configurar generadores de datos
        2. Construir arquitectura de red
        3. Configurar estrategia de optimizaciÃ³n
        4. Ejecutar entrenamiento
        5. Guardar modelo y mÃ©tricas
        6. Visualizar resultados
        """
        self.setup_data_generators()
        self.construct_network_architecture()
        self.configure_training_strategy()
        self.execute_training()
        self.persist_model_and_metrics()
        self.visualize_training_results()


def main():
    """
    FunciÃ³n principal que inicializa y ejecuta el sistema de entrenamiento.
    """
    # Construir ruta al dataset de manera robusta
    workspace_root = os.getcwd()
    dataset_path = os.path.join(
        workspace_root, 
        'practica_2', 
        'animals-dataset', 
        'animals-dataset'
    )
    
    # Validar que el dataset existe
    if not os.path.exists(dataset_path):
        print(f"âš ï¸  ERROR: No se encontrÃ³ el dataset en: {dataset_path}")
        print(f"   Por favor, verifica la ruta del dataset.")
        return
    
    # Crear instancia del entrenador
    trainer = NeuralNetworkTrainer(dataset_directory=dataset_path)
    
    # Ejecutar pipeline completo
    trainer.run_complete_pipeline()


# Punto de entrada del programa
if __name__ == "__main__":
    main()