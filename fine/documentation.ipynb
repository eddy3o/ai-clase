{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e84b0a8d",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "\n",
    "Este documento presenta un sistema completo de fine-tuning de modelos de lenguaje grandes utilizando la técnica LoRA (Low-Rank Adaptation). El sistema especializa el modelo DeepSeek-R1-Distill-Llama-8B para actuar como tutor de programación en TypeScript y conceptos de desarrollo.\n",
    "\n",
    "### 1.1 Objetivos del Proyecto\n",
    "\n",
    "- Adaptar un modelo de lenguaje base para respuestas específicas de dominio\n",
    "- Implementar fine-tuning eficiente con mínimo uso de recursos\n",
    "- Crear un tutor interactivo de programación\n",
    "- Optimizar el modelo para despliegue local\n",
    "\n",
    "### 1.2 Arquitectura del Sistema\n",
    "\n",
    "El proyecto consta de cuatro componentes principales:\n",
    "\n",
    "```\n",
    "Sistema de Fine-Tuning\n",
    "├── main.py          → Entrenamiento con LoRA\n",
    "├── inference.py     → Sistema de inferencia interactivo\n",
    "├── convert.py       → Conversión de modelos\n",
    "└── Modelfile        → Configuración para Ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1741c",
   "metadata": {},
   "source": [
    "## 2. Técnica LoRA: Low-Rank Adaptation\n",
    "\n",
    "### 2.1 Concepto Fundamental\n",
    "\n",
    "LoRA es una técnica de fine-tuning que reduce drásticamente los recursos necesarios para adaptar modelos grandes:\n",
    "\n",
    "**Problema del Fine-Tuning Tradicional:**\n",
    "- Requiere actualizar millones o miles de millones de parámetros\n",
    "- Necesita gran cantidad de memoria GPU\n",
    "- Proceso lento y costoso\n",
    "- Difícil de almacenar múltiples versiones especializadas\n",
    "\n",
    "**Solución con LoRA:**\n",
    "- Congela los pesos originales del modelo\n",
    "- Añade matrices de bajo rango entrenables\n",
    "- Actualiza solo un pequeño porcentaje de parámetros\n",
    "- Almacena adaptadores ligeros (pocos MB vs. varios GB)\n",
    "\n",
    "### 2.2 Parámetros LoRA\n",
    "\n",
    "| Parámetro | Valor | Descripción |\n",
    "|-----------|-------|-------------|\n",
    "| `r` | 8 | Rango de las matrices de bajo rango |\n",
    "| `lora_alpha` | 16 | Factor de escala para la actualización |\n",
    "| `lora_dropout` | 0.1 | Dropout aplicado a capas LoRA |\n",
    "| `target_modules` | q_proj, k_proj, v_proj, o_proj | Capas de atención modificadas |\n",
    "| `bias` | none | No entrena sesgos adicionales |\n",
    "\n",
    "### 2.3 Ventajas Técnicas\n",
    "\n",
    "**Eficiencia de Memoria:**\n",
    "- Modelo base: 8B parámetros congelados\n",
    "- Adaptadores LoRA: ~5-10M parámetros entrenables\n",
    "- Reducción: 99.9% menos parámetros a entrenar\n",
    "\n",
    "**Modularidad:**\n",
    "- Un modelo base + múltiples adaptadores\n",
    "- Intercambio rápido entre especializaciones\n",
    "- Fácil distribución y versionado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86334f",
   "metadata": {},
   "source": [
    "## 3. Modelo Base: DeepSeek-R1-Distill-Llama-8B\n",
    "\n",
    "### 3.1 Características del Modelo\n",
    "\n",
    "**Arquitectura:**\n",
    "- Familia: Llama (Meta AI)\n",
    "- Tamaño: 8 mil millones de parámetros\n",
    "- Tipo: Modelo causal de lenguaje (autoregresivo)\n",
    "- Contexto: Ventana de contexto extendida\n",
    "\n",
    "**Especialización:**\n",
    "- Destilado de DeepSeek-R1\n",
    "- Optimizado para razonamiento\n",
    "- Balance entre rendimiento y tamaño\n",
    "\n",
    "### 3.2 Cuantización 4-bit\n",
    "\n",
    "El sistema utiliza cuantización para reducir requisitos de hardware:\n",
    "\n",
    "**Técnica BitsAndBytes:**\n",
    "```python\n",
    "load_in_4bit=True\n",
    "bnb_4bit_quant_type=\"nf4\"        # NormalFloat4\n",
    "bnb_4bit_compute_dtype=float16   # Cálculos en FP16\n",
    "bnb_4bit_use_double_quant=True   # Doble cuantización\n",
    "```\n",
    "\n",
    "**Impacto:**\n",
    "- Memoria: ~5-6 GB VRAM vs. ~16 GB sin cuantización\n",
    "- Velocidad: Mínima degradación con GPUs modernas\n",
    "- Precisión: Pérdida menor al 1% en métricas estándar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930bc4e",
   "metadata": {},
   "source": [
    "## 4. Preparación del Dataset\n",
    "\n",
    "### 4.1 Estructura de Datos\n",
    "\n",
    "El dataset utiliza formato JSONL (JSON Lines) con estructura instrucción-respuesta:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Pregunta o tarea del usuario\",\n",
    "  \"response\": \"Respuesta esperada del modelo\"\n",
    "}\n",
    "```\n",
    "\n",
    "### 4.2 Dominio del Dataset\n",
    "\n",
    "El dataset contiene 278 pares de instrucción-respuesta sobre:\n",
    "\n",
    "**Temas Principales:**\n",
    "- Sistema de tipos de TypeScript\n",
    "- Interfaces y tipos personalizados\n",
    "- Enums y union types\n",
    "- Genéricos y utility types\n",
    "- Corrección de errores de tipo\n",
    "- Patrones avanzados de TypeScript\n",
    "\n",
    "### 4.3 Formato de Prompt\n",
    "\n",
    "Cada ejemplo se formatea con un template consistente:\n",
    "\n",
    "```python\n",
    "prompt = f\"Instrucción: {instruction}\\nRespuesta:\"\n",
    "texto_completo = prompt + response\n",
    "```\n",
    "\n",
    "**Beneficios:**\n",
    "- Estructura predecible para el modelo\n",
    "- Fácil parsing durante inferencia\n",
    "- Consistencia en formato de entrada/salida\n",
    "\n",
    "### 4.4 Tokenización\n",
    "\n",
    "El proceso de tokenización incluye:\n",
    "\n",
    "- Conversión de texto a tokens numéricos\n",
    "- Truncamiento a longitud máxima\n",
    "- Padding token configurado como EOS\n",
    "- Preservación del formato instrucción-respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb2ee8b",
   "metadata": {},
   "source": [
    "## 5. Proceso de Entrenamiento\n",
    "\n",
    "### 5.1 Configuración de Hiperparámetros\n",
    "\n",
    "| Parámetro | Valor | Justificación |\n",
    "|-----------|-------|---------------|\n",
    "| `per_device_train_batch_size` | 1 | Limita uso de memoria con modelo 8B |\n",
    "| `gradient_accumulation_steps` | 10 | Simula batch size de 10 |\n",
    "| `num_train_epochs` | 6 | Balance entre ajuste y sobreajuste |\n",
    "| `fp16` | True | Entrenamiento en precisión mixta |\n",
    "| `logging_steps` | 50 | Frecuencia de reporte de métricas |\n",
    "| `save_steps` | 500 | Checkpoints periódicos |\n",
    "\n",
    "### 5.2 Acumulación de Gradientes\n",
    "\n",
    "Técnica para simular batches grandes con memoria limitada:\n",
    "\n",
    "**Funcionamiento:**\n",
    "1. Procesa 1 ejemplo (forward pass)\n",
    "2. Calcula gradientes (backward pass)\n",
    "3. Acumula gradientes sin actualizar pesos\n",
    "4. Repite 10 veces\n",
    "5. Actualiza pesos con gradiente acumulado\n",
    "\n",
    "**Ventaja:** Batch efectivo de 10 con memoria de 1\n",
    "\n",
    "### 5.3 Precision Mixta FP16\n",
    "\n",
    "Combina diferentes precisiones numéricas:\n",
    "\n",
    "- **FP16**: Cálculos rápidos en GPU\n",
    "- **FP32**: Acumuladores para estabilidad\n",
    "\n",
    "**Beneficios:**\n",
    "- 2x más rápido en GPUs modernas\n",
    "- 50% menos uso de memoria\n",
    "- Mínima pérdida de precisión\n",
    "\n",
    "### 5.4 Objetivo de Entrenamiento\n",
    "\n",
    "El modelo se entrena con **Causal Language Modeling**:\n",
    "\n",
    "- Predice el siguiente token dada la secuencia previa\n",
    "- Función de pérdida: Cross-entropy sobre vocabulario\n",
    "- Enmascaramiento: Solo calcula pérdida en tokens de respuesta\n",
    "\n",
    "### 5.5 Monitoreo del Entrenamiento\n",
    "\n",
    "Durante el entrenamiento se observa:\n",
    "\n",
    "- **Loss (pérdida)**: Debe disminuir consistentemente\n",
    "- **GPU utilization**: Verificar uso eficiente\n",
    "- **Memory**: Confirmar que no hay OOM (Out Of Memory)\n",
    "- **Steps/second**: Velocidad de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2188f0",
   "metadata": {},
   "source": [
    "## 6. Sistema de Inferencia\n",
    "\n",
    "### 6.1 Carga del Modelo\n",
    "\n",
    "El sistema carga el modelo en tres pasos:\n",
    "\n",
    "```python\n",
    "# 1. Modelo base cuantizado\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# 2. Adaptadores LoRA\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora-tutor\")\n",
    "\n",
    "# 3. Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(...)\n",
    "```\n",
    "\n",
    "### 6.2 Parámetros de Generación\n",
    "\n",
    "| Parámetro | Valor | Efecto |\n",
    "|-----------|-------|--------|\n",
    "| `max_new_tokens` | 515 | Longitud máxima de respuesta |\n",
    "| `temperature` | 0.003 | Generación casi determinista |\n",
    "| `do_sample` | True | Habilita muestreo probabilístico |\n",
    "| `top_p` | 0.85 | Nucleus sampling (Top-P) |\n",
    "| `repetition_penalty` | 1.15 | Penaliza repeticiones |\n",
    "\n",
    "### 6.3 Estrategias de Decodificación\n",
    "\n",
    "#### Temperature\n",
    "\n",
    "Controla la aleatoriedad de las predicciones:\n",
    "\n",
    "- **0.003**: Casi greedy, muy determinista\n",
    "- Selecciona tokens con mayor probabilidad\n",
    "- Ideal para respuestas técnicas precisas\n",
    "\n",
    "#### Nucleus Sampling (Top-P)\n",
    "\n",
    "Selecciona del conjunto más probable de tokens:\n",
    "\n",
    "- **top_p = 0.85**: Considera tokens que suman 85% probabilidad\n",
    "- Elimina opciones muy improbables\n",
    "- Balance entre coherencia y diversidad\n",
    "\n",
    "#### Repetition Penalty\n",
    "\n",
    "Penaliza tokens ya generados:\n",
    "\n",
    "- **1.15**: Penalización moderada\n",
    "- Previene bucles de repetición\n",
    "- Mantiene variedad léxica\n",
    "\n",
    "### 6.4 Flujo de Inferencia\n",
    "\n",
    "Proceso completo de generación:\n",
    "\n",
    "1. **Entrada del usuario**: Pregunta en lenguaje natural\n",
    "2. **Formateo**: Añadir template de instrucción\n",
    "3. **Tokenización**: Convertir a IDs numéricos\n",
    "4. **Generación**: Producir tokens autorregresvamente\n",
    "5. **Decodificación**: Convertir IDs a texto\n",
    "6. **Post-procesamiento**: Extraer solo la respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892e526b",
   "metadata": {},
   "source": [
    "## 7. Conversión y Despliegue\n",
    "\n",
    "### 7.1 Fusión de Adaptadores\n",
    "\n",
    "El script `convert.py` fusiona LoRA con el modelo base:\n",
    "\n",
    "**Proceso:**\n",
    "```python\n",
    "# Cargar modelo base + LoRA\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "\n",
    "# Fusionar\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Guardar\n",
    "merged_model.save_pretrained(output_path)\n",
    "```\n",
    "\n",
    "**Resultado:**\n",
    "- Modelo unificado sin dependencias de PEFT\n",
    "- Más rápido en inferencia\n",
    "- Compatible con herramientas estándar\n",
    "\n",
    "### 7.2 Conversión a GGUF\n",
    "\n",
    "GGUF es un formato optimizado para inferencia en CPU:\n",
    "\n",
    "**Características:**\n",
    "- Cuantización adicional (Q4, Q5, Q8)\n",
    "- Optimizado para llama.cpp\n",
    "- Ejecución eficiente en CPU\n",
    "- Compatible con Ollama\n",
    "\n",
    "**Limitación:**\n",
    "- Puede fallar con modelos ya cuantizados en 4-bit\n",
    "- Requiere modelo en FP16/FP32 para conversión óptima\n",
    "\n",
    "### 7.3 Integración con Ollama\n",
    "\n",
    "El `Modelfile` define la configuración para Ollama:\n",
    "\n",
    "```\n",
    "FROM deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
    "ADAPTER ./lora-tutor\n",
    "SYSTEM \"Prompt de sistema personalizado\"\n",
    "```\n",
    "\n",
    "**Ventajas de Ollama:**\n",
    "- API REST simple\n",
    "- Gestión de modelos locales\n",
    "- Compatible con múltiples frameworks\n",
    "- Interfaz web incluida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea15ac",
   "metadata": {},
   "source": [
    "## 8. Optimizaciones de Memoria\n",
    "\n",
    "### 8.1 Cuantización en Cascada\n",
    "\n",
    "El sistema aplica cuantización en múltiples niveles:\n",
    "\n",
    "**Nivel 1: Modelo Base (4-bit NF4)**\n",
    "- Reduce 8B parámetros de FP16 a 4-bit\n",
    "- Ahorro: ~75% de memoria\n",
    "- De ~16 GB a ~4 GB\n",
    "\n",
    "**Nivel 2: Doble Cuantización**\n",
    "- Cuantiza los factores de escala\n",
    "- Ahorro adicional: ~10-15%\n",
    "- Mínimo impacto en calidad\n",
    "\n",
    "### 8.2 Gestión de Dispositivos\n",
    "\n",
    "Estrategias para distribución de memoria:\n",
    "\n",
    "```python\n",
    "# Automática\n",
    "device_map=\"auto\"  # Distribuye capas óptimamente\n",
    "\n",
    "# Manual\n",
    "max_memory={0: \"6GB\"}  # Limita uso de GPU 0\n",
    "```\n",
    "\n",
    "**Offloading:**\n",
    "- Capas menos usadas a CPU/Disco\n",
    "- Mantiene capas críticas en GPU\n",
    "- Trade-off memoria vs. velocidad\n",
    "\n",
    "### 8.3 Configuración de VRAM\n",
    "\n",
    "Requisitos según configuración:\n",
    "\n",
    "| Configuración | VRAM | Velocidad |\n",
    "|---------------|------|----------|\n",
    "| 4-bit + LoRA | 5-6 GB | Rápida |\n",
    "| 8-bit | 8-10 GB | Media |\n",
    "| FP16 | 16+ GB | Muy rápida |\n",
    "| CPU Offload | 2-3 GB | Lenta |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71eb07",
   "metadata": {},
   "source": [
    "## 9. Evaluación del Modelo\n",
    "\n",
    "### 9.1 Métricas de Calidad\n",
    "\n",
    "**Durante Entrenamiento:**\n",
    "- Loss de entrenamiento: Indicador de ajuste\n",
    "- Perplexity: Medida de incertidumbre del modelo\n",
    "- Convergencia: Estabilización de la pérdida\n",
    "\n",
    "**Durante Inferencia:**\n",
    "- Relevancia de respuestas\n",
    "- Precisión técnica\n",
    "- Coherencia en formato\n",
    "- Ausencia de alucinaciones\n",
    "\n",
    "### 9.2 Evaluación Cualitativa\n",
    "\n",
    "Aspectos a verificar manualmente:\n",
    "\n",
    "**Conocimiento del Dominio:**\n",
    "- Corrección de conceptos de TypeScript\n",
    "- Sintaxis apropiada en ejemplos\n",
    "- Terminología técnica precisa\n",
    "\n",
    "**Capacidad Pedagógica:**\n",
    "- Explicaciones claras y concisas\n",
    "- Ejemplos prácticos relevantes\n",
    "- Estructura de respuesta didáctica\n",
    "\n",
    "**Consistencia:**\n",
    "- Mismo formato en todas las respuestas\n",
    "- Tono profesional uniforme\n",
    "- Adherencia al template\n",
    "\n",
    "### 9.3 Casos de Prueba\n",
    "\n",
    "Categorías de preguntas para validación:\n",
    "\n",
    "1. **Definiciones**: \"¿Qué es una interfaz?\"\n",
    "2. **Ejemplos**: \"Muestra un ejemplo de genéricos\"\n",
    "3. **Correcciones**: \"Corrige este error de tipo\"\n",
    "4. **Comparaciones**: \"Diferencia entre type y interface\"\n",
    "5. **Aplicaciones**: \"¿Cuándo usar enums?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9fd23",
   "metadata": {},
   "source": [
    "## 10. Arquitectura del Sistema\n",
    "\n",
    "### 10.1 Componentes y Flujos\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│          FASE DE ENTRENAMIENTO                  │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│                                                 │\n",
    "│  Dataset (JSONL) → Tokenización → Batching    │\n",
    "│         ↓                                       │\n",
    "│  Modelo Base (8B) + Cuantización 4-bit        │\n",
    "│         ↓                                       │\n",
    "│  Configuración LoRA (r=8, alpha=16)           │\n",
    "│         ↓                                       │\n",
    "│  Entrenamiento (6 épocas, batch_size=10)      │\n",
    "│         ↓                                       │\n",
    "│  Guardado de Adaptadores (~10 MB)             │\n",
    "│                                                 │\n",
    "└─────────────────────────────────────────────────┘\n",
    "\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│          FASE DE INFERENCIA                     │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│                                                 │\n",
    "│  Pregunta Usuario → Formateo Template         │\n",
    "│         ↓                                       │\n",
    "│  Modelo Base + Adaptadores LoRA               │\n",
    "│         ↓                                       │\n",
    "│  Generación (temp=0.003, top_p=0.85)          │\n",
    "│         ↓                                       │\n",
    "│  Post-procesamiento y Visualización           │\n",
    "│                                                 │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 10.2 Estructura de Archivos\n",
    "\n",
    "```\n",
    "fine/\n",
    "├── main.py                    # Script de entrenamiento\n",
    "├── inference.py               # Sistema interactivo\n",
    "├── convert.py                 # Fusión y conversión\n",
    "├── Modelfile                  # Config Ollama\n",
    "├── respuestas_fixed.jsonl     # Dataset de entrenamiento\n",
    "└── lora-tutor/                # Adaptadores entrenados\n",
    "    ├── adapter_config.json    # Configuración LoRA\n",
    "    ├── adapter_model.safetensors  # Pesos LoRA\n",
    "    └── README.md              # Metadatos del modelo\n",
    "```\n",
    "\n",
    "### 10.3 Dependencias Principales\n",
    "\n",
    "```python\n",
    "transformers>=4.30.0    # Framework de modelos\n",
    "peft>=0.4.0            # Parameter-Efficient Fine-Tuning\n",
    "bitsandbytes>=0.40.0   # Cuantización\n",
    "accelerate>=0.20.0     # Optimizaciones de hardware\n",
    "datasets>=2.12.0       # Manejo de datos\n",
    "torch>=2.0.0           # Framework de deep learning\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
